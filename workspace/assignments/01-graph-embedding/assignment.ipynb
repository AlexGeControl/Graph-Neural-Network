{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 第四章作业","metadata":{}},{"cell_type":"markdown","source":"本次作业我们加强对图嵌入模型的实践，具体地，我们需要利用gensim.Word2Vec模型来实现Node2Vec模型。","metadata":{}},{"cell_type":"markdown","source":"在开始实践之前，首先请同学回答两个问题：\n1. Node2Vec模型中的p参数和q参数各代表什么意思？\n2. 在Node2Vec采样随机游走时，如果我们鼓励随机游走访问之前被采样过的节点，我们应该如何调节p或者q？","metadata":{}},{"cell_type":"markdown","source":"## 代码填空 - Node2Vec采样随机游走","metadata":{}},{"cell_type":"code","source":"# 我们导入需要用到的工具包，如果出现ImportError麻烦大家自己安装一下对应的工具包\nimport numba\nimport numpy as np\nimport scipy.sparse as sp\nfrom gensim.models import Word2Vec\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import accuracy_score","metadata":{},"execution_count":1,"outputs":[{"name":"stderr","output_type":"stream","text":"/Users/jinwei/anaconda3/envs/gnn/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"}]},{"cell_type":"code","source":"def node2vec(adj, embedding_dim=64, walk_length=30, walks_per_node=10,\n                  workers=8, window_size=10, num_neg_samples=1, p=4, q=1):\n    \"\"\"\n    参数说明\n    -------------\n    adj : 图的邻接矩阵\n    embedding_dim : 图嵌入的维度\n    walk_length : 随机游走的长度\n    walks_per_node : 每个节点采样多少个随机游走\n    workers: word2vec模型使用的线程数量\n    window_size: word2vec模型中使用的窗口大小\n    num_neg_samples : 负样本的数量\n    p: node2vec的p参数\n    q: node2vec的q参数\n    \"\"\"\n    walks = sample_n2v_random_walks(adj, walk_length, walks_per_node, p=p, q=q) # 利用随机游走提取共现信息\n    walks = [list(map(str, walk)) for walk in walks]\n    model = Word2Vec(walks, vector_size=embedding_dim, \n                     negative=num_neg_samples, compute_loss=True)   # 映射函数、重构器、目标\n    embedding = model.wv.vectors[np.fromiter(map(int, model.wv.index_to_key), np.int32).argsort()] # 从词向量中取出节点嵌入\n    return embedding\n\ndef sample_n2v_random_walks(adj, walk_length, walks_per_node, p, q):\n    \"\"\"\n    返回值的类型\n    -------\n    walks : np.ndarray, shape [num_walks * num_nodes, walk_length]\n        采样后的随机游走\n    \"\"\"\n    adj = sp.csr_matrix(adj)\n    random_walks = _n2v_random_walk(adj.indptr,\n                                    adj.indices,\n                                    walk_length,\n                                    walks_per_node,\n                                    p,\n                                    q)\n    return random_walks ","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"### 不用numba加速的版本\ndef _n2v_random_walk(indptr,\n                    indices,\n                    walk_length,\n                    walks_per_node,\n                    p,\n                    q):\n    N = len(indptr) - 1 # 节点数量\n    final_walks = [] # 存储所有的随机游走\n    for _ in range(walks_per_node):\n        for n in range(N):\n            walk = [n]\n            ######################################\n            #\n            #      同学们需要自己完成这部分代码\n            #\n            #\n            ######################################\n            final_walks.append(walk)\n    return np.array(final_walks)\n\n\n# ### 用numba加速的版本\n# # 建议debug阶段把下面这行注释掉，debug通过后再把取消下面这行的注释\n# # @numba.jit(nopython=True)\n# def _n2v_random_walk(indptr,\n#                     indices,\n#                     walk_length,\n#                     walks_per_node,\n#                     p,\n#                     q):\n#     N = len(indptr) - 1 # 节点数量\n\n#     for _ in range(walks_per_node):\n#         for n in range(N):\n#             walk = [n]\n#             ######################################\n#             #\n#             #      同学们需要自己完成这部分代码\n#             #\n#             #\n#             ######################################\n#             yield walk # 用yield来构造一个generator","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"在上面的过程中，大家会需要根据概率来采样数组中的元素，请大家直接使用下面这个`random_choice`函数，它跟`numpy.random.choice`类似，但是这个函数可以支持在numba中使用不同概率来采样。详情可见这个页面<https://github.com/numba/numba/issues/2539#issuecomment-507306369>","metadata":{}},{"cell_type":"code","source":"@numba.jit(nopython=True)\ndef random_choice(arr, p):\n    \"\"\"\n    参数说明\n    ----------\n    arr : 1-D 数组\n    p : 数组中每个元素对应的概率\n    \n    返回值\n    -------\n    samples : 采样后的元素\n    \"\"\"\n    return arr[np.searchsorted(np.cumsum(p), np.random.random(), side=\"right\")]","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 代码填空- 完成测试代码","metadata":{}},{"cell_type":"code","source":"from torch_geometric.datasets import Planetoid\nfrom torch_geometric.utils import to_scipy_sparse_matrix\ndataset = Planetoid(root='./data', name='Cora')# 将数据保存在data文件夹下\ndata = dataset[0]\nadj = to_scipy_sparse_matrix(data.edge_index)","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"embedding = node2vec(adj, embedding_dim=64, p=0.5, q=0.5)\nembedding.shape","metadata":{},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(2708, 64)"},"metadata":{}}]},{"cell_type":"code","source":"# 请大家完成下面这个测试函数\ndef evaluate_node_classification(embedding_matrix, labels, train_mask, \n                                 test_mask, normalize_embedding=True, max_iter=1000):\n        \n    \"\"\"训练一个线性模型（比如逻辑回归模型）来预测节点的标签\n    \n    返回值说明\n    ----\n    preds: 模型预测的标签\n    test_acc: 模型预测的准确率\n    \"\"\"\n    ######################################\n    #\n    #      同学们需要自己完成这部分代码\n    #\n    #\n    ######################################   \n    return preds, test_acc","metadata":{},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"preds, test_acc = evaluate_node_classification(embedding, data.y, data.train_mask, data.test_mask)\nprint('Test Acc: %.4f' % test_acc)","metadata":{},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"Test Acc: 0.6890\n"}]},{"cell_type":"markdown","source":"## 拓展问题","metadata":{}},{"cell_type":"markdown","source":"请同学们调节p参数和q参数，对比Node2Vec模型的效果和DeepWalk模型(p=q=1)的效果。","metadata":{}}]}