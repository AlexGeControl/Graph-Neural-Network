{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 第五章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业我们加强对图神经网络的实践，包括GAT和GraphSAGE。具体地，我们需要（1）实现PyG中的GATConv，（2）利用PyG中的采样（Sampling）功能来完成GraphSAGE。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GAT 代码填空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT里的聚合过程可以表示为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{x}^{\\prime}_i = \\alpha_{i,i}\\mathbf{\\Theta}\\mathbf{x}_{i} +\n",
    "\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i,j}\\mathbf{\\Theta}\\mathbf{x}_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中节点i和节点j之间的注意力分数为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_{i,j} =\n",
    "\\frac{\n",
    "\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_j]\n",
    "\\right)\\right)}\n",
    "{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{ i \\}}\n",
    "\\exp\\left(\\mathrm{LeakyReLU}\\left(\\mathbf{a}^{\\top}\n",
    "[\\mathbf{\\Theta}\\mathbf{x}_i \\, \\Vert \\, \\mathbf{\\Theta}\\mathbf{x}_k]\n",
    "\\right)\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码改编自PyG某一版本的GATConv实现。我做了诸多简化来让它简单易读且适应于当前版本。\n",
    "\n",
    "注：这里我们没有用最新版PyG中GATConv的实现，因为最新版本的GATConv不是特别好懂。\n",
    "\n",
    "实际上我们参考的是1.3.2版本的GATConv，见该链接：https://github.com/pyg-team/pytorch_geometric/blob/881d5ba2aefc26328eeeaa17fd7ef6daaae06ef4/torch_geometric/nn/conv/gat_conv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "import torch.optim as optim\n",
    "\n",
    "class GATConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    参数说明\n",
    "    ------   \n",
    "    in_channels: 输入神经元的数量\n",
    "    out_channels: 输出神经元的数量\n",
    "    heads: 注意力机制head的数量\n",
    "    concat: 如果concat是True，那么最后的输出就是拼接每个head的输出；如果concat是False，\n",
    "        那么最后的输出就是对每个head的输出求平均.\n",
    "    negative_slope: LeakyReLU中(-∞,0)的部分的斜率。\n",
    "    dropout: 对注意力分数的dropout概率。\n",
    "    bias: 偏置项\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels,\n",
    "                 out_channels, heads=1, concat=True,\n",
    "                 negative_slope=0.2, dropout=0.0,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super(GATConv, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin = Linear(in_channels, heads * out_channels,\n",
    "                              bias=False, weight_initializer='glorot')\n",
    "\n",
    "        # 注意力机制中的参数\n",
    "        self.att = Parameter(torch.Tensor(1, heads, 2*out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"初始化参数\"\"\"\n",
    "        self.lin.reset_parameters()\n",
    "        glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "    \n",
    "    def forward(self, x, edge_index, size=None):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        output = self.propagate(edge_index, size=size, x=x) # 得到聚合信息后的节点特征\n",
    "        \n",
    "        if self.concat is True:\n",
    "            #################\n",
    "            #### 代码填空 ####\n",
    "            ################\n",
    "        else:\n",
    "            #################\n",
    "            #### 代码填空 ####\n",
    "            ################\n",
    "\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "\n",
    "    def message(self, edge_index_i, x_i, x_j, size_i):\n",
    "        \"\"\"计算注意力分数。\n",
    "    \n",
    "        参数说明\n",
    "        ----\n",
    "        edge_index_i: 边的序号的第一维，对应x_i的邻居节点\n",
    "        x_i: source节点的节点特征\n",
    "        x_j: target节点的节点特征\n",
    "        size_i: source节点的节点数量\n",
    "        \"\"\"\n",
    "        \n",
    "        ###############################################\n",
    "        #### 代码填空，计算softmax之前的注意力分数alpha ####\n",
    "        ###############################################\n",
    "        \n",
    "        alpha = softmax(src=alpha, index=edge_index_i, num_nodes=size_i)\n",
    "        \n",
    "        # 对注意力分数alpha进行dropout\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        ##################################\n",
    "        #### 代码填空，完成需要返回的变量 ####\n",
    "        #################################\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    \"\"\" 2层GAT.\n",
    "    \n",
    "    参数说明\n",
    "    ----------\n",
    "    nfeat : 输入特征的维度\n",
    "    nhid : 隐藏神经元的数量\n",
    "    nclass : 输出神经元的数量，也即类别的数量\n",
    "    heads: 注意力机制中的head数量\n",
    "    output_heads: 输出层的head数量\n",
    "    dropout : dropout中的概率\n",
    "    with_bias: 是否带有偏置项\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, heads=8, output_heads=1, dropout=0.5, with_bias=True):\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            nfeat,\n",
    "            nhid,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            nhid * heads,\n",
    "            nclass,\n",
    "            heads=output_heads,\n",
    "            concat=False,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.elu(self.conv1(x, edge_index)) # 按照原论文的设置，我们使用ELu作为激活函数\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"初始化GAT的参数.\n",
    "        \"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, lr=0.01, weight_decay=5e-4, epochs=200):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    #################\n",
    "    #### 代码填空 ####\n",
    "    ################\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    \"\"\"测试模型在测试集上的性能\"\"\"\n",
    "    #################\n",
    "    #### 代码填空 ####\n",
    "    ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 1.930566430091858\n",
      "Epoch 10, training loss: 0.36536264419555664\n",
      "Epoch 20, training loss: 0.24459099769592285\n",
      "Epoch 30, training loss: 0.2327195256948471\n",
      "Epoch 40, training loss: 0.241203173995018\n",
      "Epoch 50, training loss: 0.19684189558029175\n",
      "Epoch 60, training loss: 0.192429780960083\n",
      "Epoch 70, training loss: 0.25000977516174316\n",
      "Epoch 80, training loss: 0.26480817794799805\n",
      "Epoch 90, training loss: 0.1946670413017273\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='./data', name='Cora') # 将数据保存在data文件夹下\n",
    "data = dataset[0]\n",
    "nclass = data.y.max().item()+1\n",
    "gat = GAT(nfeat=data.x.shape[1],\n",
    "      nhid=8, heads=8, nclass=nclass)\n",
    "train(gat, data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7546 accuracy= 0.7980\n"
     ]
    }
   ],
   "source": [
    "preds, output, acc = test(gat, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GraphSAGE 代码填空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSAGE的核心部分是采样（sampling）。我们利用PyG提供的NeighborSampler来实现采样功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分的代码填空很少，主要是希望同学们能够通过例子来学会使用NeighborSampler。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外需要注意的是GraphSAGE中的聚合方式，它有两个变换矩阵：\n",
    "\n",
    "$$ \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "dataset = Planetoid(root='./data', name='Cora') # 将数据保存在data文件夹下\n",
    "data = dataset[0]\n",
    "nclass = data.y.max().item()+1\n",
    "\n",
    "size=[10,5] # 表示第一层采样10个邻居，第二层采样5个邻居\n",
    "train_idx = torch.arange(data.num_nodes)[data.train_mask]\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,\n",
    "                               sizes=sizes, batch_size=128,\n",
    "                               shuffle=True, num_workers=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    \"\"\" 2层GraphSAGE\n",
    "    \n",
    "    参数说明\n",
    "    ----------\n",
    "    nfeat : 输入特征的维度\n",
    "    nhid : 隐藏神经元的数量\n",
    "    nclass : 输出神经元的数量，也即类别的数量\n",
    "    dropout : dropout中的概率\n",
    "    with_bias: 是否带有偏置项\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(nfeat, nhid, bias=with_bias))\n",
    "        self.convs.append(SAGEConv(nhid, nclass, bias=with_bias))\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"初始化模型参数\"\"\"\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        \"\"\"对应于neighborsampler的前向传播\"\"\"\n",
    "        num_layers = len(adjs)\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]  # x_target是目标节点，最后一层的目标节点就是带标签的节点\n",
    "            x = self.convs[i]((x, x_target), \n",
    "                              edge_index) # x是邻居节点的特征，x_target是目标节点的特征，它们对应不同的特征变换矩阵\n",
    "            if i != num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x.log_softmax(dim=-1)       \n",
    "            \n",
    "    def inference(self, data):\n",
    "        \"\"\"模型测试阶段的前向传播，不采样邻居节点，直接使用所有的邻居。可参考GCN中forward()的实现\"\"\"\n",
    "        #################\n",
    "        #### 代码填空 ####\n",
    "        ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, device='cpu', lr=0.01, weight_decay=5e-4):\n",
    "    \"\"\"训练阶段，这部分我们就不再设置填空作业，而是选择让同学们自行理解\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    x = data.x.to(device)\n",
    "    y = data.y.squeeze().to(device)\n",
    "    \n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        for batch_size, n_id, adjs in train_loader:\n",
    "            # `n_id`是被采样的节点（包含了有标签的节点，和无标签的邻居节点）\n",
    "            # `adjs`里面对应了每一层采样的邻接矩阵，包含了`(edge_index, e_id, size)`\n",
    "            adjs = [adj.to(device) for adj in adjs]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x[n_id], adjs)\n",
    "            loss = F.nll_loss(out, y[n_id[:batch_size]]) # n_id[:batch_size]表示的采样的带标签的节点\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        loss = total_loss / len(train_loader)\n",
    "        if it % 10 ==0:\n",
    "            print('Epoch:', it, 'training loss:', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model):\n",
    "    \"\"\"测试模型在测试集上的性能\"\"\"\n",
    "    model.eval() # eval()把dropout的概率设置为0（不使用dropout）\n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model.inference(data) # 得到模型输出\n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) # 得到预测值\n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() # 得到准确率\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 training loss: 3.847064733505249\n",
      "Epoch: 10 training loss: 0.6190792322158813\n",
      "Epoch: 20 training loss: 0.10876146703958511\n",
      "Epoch: 30 training loss: 0.10637849196791649\n",
      "Epoch: 40 training loss: 0.13567957282066345\n",
      "Epoch: 50 training loss: 0.04347763862460852\n",
      "Epoch: 60 training loss: 0.0496326326392591\n",
      "Epoch: 70 training loss: 0.09714920818805695\n",
      "Epoch: 80 training loss: 0.04202270694077015\n",
      "Epoch: 90 training loss: 0.0426302095875144\n"
     ]
    }
   ],
   "source": [
    "sage = GraphSAGE(nfeat=data.x.shape[1], nhid=16, nclass=nclass)\n",
    "train(sage, train_loader, epochs=100, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6927 accuracy= 0.7980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.798"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred, output, acc_test = test(sage)\n",
    "acc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
