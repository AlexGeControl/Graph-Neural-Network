{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业-隐式反馈的推荐系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业中，我们使用对前面的LightGCN进行如下两个改动:\n",
    "- 用稀疏矩阵的形式构建Movielens图\n",
    "- 利用隐式反馈实现BPR损失函数\n",
    "- 不依赖PyG直接实现LightGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "课程里我们用到的Movielens数据集有明确的用户评分，这是用户的显式反馈。我们训练推荐模型的目标是准确预测用户对他们观看的电影的评分。这种关注评分的方式忽略了考虑用户首先选择观看的电影的重要性，并且没有考虑用户没有评分的电影。缺失的评分更可能是负面的，因为用户往往会先对电影进行一次筛选，再去要观看、评分。选择电影时，用户往往只会观看你认为你会喜欢的电影，而不会去观看他们认为会讨厌的电影。这导致用户只会提交一开始他们希望喜欢的事物的评分，而不会评价不会喜欢的事物。这些没有被评分反映出来的信息，被称为隐式反馈（implicit feedback）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 用稀疏矩阵的形式构建Movielens图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们把课件里的部分代码摘抄过来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/MovieLens/raw/ml-latest-small/ratings.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实践课里面，我们有用户的评分数据，我们过滤掉评分小于3的数据(去掉低评分)，来构造一个隐式反馈的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81763 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[81763 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.rating >= 3]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户数量: 609 物品数量: 8452\n"
     ]
    }
   ],
   "source": [
    "user_mapping = {idx: i for i, idx in enumerate(df['userId'].unique())}\n",
    "movie_mapping = {idx: i for i, idx in enumerate(df['movieId'].unique())}\n",
    "num_users = len(user_mapping)\n",
    "num_movies = len(movie_mapping)\n",
    "print('用户数量:', num_users, '物品数量:', num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_src = [user_mapping[idx] for idx in df['userId']] # 起始节点\n",
    "movie_dst = [movie_mapping[idx]+num_users for idx in df['movieId']] # 终止节点\n",
    "edge_index = torch.tensor([user_src, movie_dst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分训练集、验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_N = edge_index.shape[1]\n",
    "indicies_perm = torch.randperm(_N)\n",
    "idx_train = indicies_perm[: int(0.8*_N)]\n",
    "train_edge_index = edge_index[:, idx_train]\n",
    "\n",
    "idx_val = indicies_perm[int(0.8*_N): int(0.9*_N)]\n",
    "val_edge_index = edge_index[:, idx_val]\n",
    "\n",
    "idx_test = indicies_perm[int(0.9*_N): ]\n",
    "test_edge_index = edge_index[:, idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_undirected(edge_index):\n",
    "    edge_index_rev = torch.stack([edge_index[1], edge_index[0]]) # 反向边\n",
    "    edge_index_sym = torch.cat([edge_index, edge_index_rev], dim=1)\n",
    "    return edge_index_sym\n",
    "\n",
    "train_graph_edge_index = to_undirected(train_edge_index)\n",
    "test_graph_edge_index = to_undirected(torch.cat([train_edge_index, val_edge_index], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**完成下面的代码填空（之前的课程里有讲过）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def construct_sparse_matrix(data, row, col, N):\n",
    "    \"\"\"\n",
    "    参数说明：\n",
    "    ---\n",
    "    data:稀疏矩阵的元素\n",
    "    row: 非零元素的行序号\n",
    "    col: 非零元素的列序号\n",
    "    N: 矩阵的维度大小\n",
    "    \"\"\"\n",
    "    return sp.csr_matrix((data, (row, col)), shape=(N, N))\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"标准化：A' = (D)^-1/2 * ( A ) * (D)^-1/2\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    #######代码填空#####  \n",
    "    ################## \n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"将scipy.sparse形式的稀疏矩阵变成torch里的sparse tensor，\n",
    "       我们需要得到三个变量，row, col和data，它们表示的其实就是\n",
    "       edge_index和edge_weight。\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    #######代码填空#####  \n",
    "    ################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_norm(row, col, N):\n",
    "    \"\"\"\n",
    "    参数说明：\n",
    "    ---\n",
    "    row: 非零元素的行序号\n",
    "    col: 非零元素的列序号\n",
    "    N: 矩阵的维度大小\n",
    "    \"\"\"\n",
    "    vals = np.array([1]*len(row))\n",
    "    adj = construct_sparse_matrix(vals, row, col, N)\n",
    "    adj_norm = normalize_adj(adj)\n",
    "    adj_norm = sparse_mx_to_torch_sparse_tensor(adj_norm)\n",
    "    return adj_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4fab2049d4f5>:19: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1/2).flatten() # 得到度矩阵D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 609,  611,  612,  ...,  608,  608,  608],\n",
       "                       [   0,    0,    0,  ..., 9056, 9057, 9060]]),\n",
       "       values=tensor([0.0059, 0.0086, 0.0060,  ..., 0.0331, 0.0331, 0.0331]),\n",
       "       size=(9061, 9061), nnz=130820, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_adj_norm = get_adj_norm(train_graph_edge_index[0], train_graph_edge_index[1], N=num_movies+num_users)\n",
    "train_adj_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4fab2049d4f5>:19: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1/2).flatten() # 得到度矩阵D\n"
     ]
    }
   ],
   "source": [
    "test_adj_norm = get_adj_norm(test_graph_edge_index[0], test_graph_edge_index[1], N=num_movies+num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 实现BPR损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPR损失函数经常用于训练推荐系统模型。它鼓励已观察到的节点对的预测值，应该大于那些未被观测到的节点对的预测值。给定节点对的排序分数$\\hat{y}_{ui}$，BPR损失函数表示为：\n",
    "\n",
    "$$L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
    "\\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
    " + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2$$\n",
    "\n",
    "其中$u$表示用户，$i,j$表示物品，$\\lambda$控制$L_2$正则项。实际中我们取BPR的均值来作为最后的损失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们处理一下评分数据rating。后面我们要用到BPR损失函数，它不是一个分类损失，我们把小于等于3的评分作为负样本，大于3的评分作为正样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = train_edge_index.numpy().T\n",
    "train_mat_set = set()\n",
    "for x in train_mat:\n",
    "    train_mat_set.add((x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(positives, negatives, parameters=None, lambda_reg=0):\n",
    "    \"\"\"求BPR损失对于所有用户的平均值\"\"\"\n",
    "    ###################\n",
    "    #######代码填空#####  \n",
    "    ################## \n",
    "\n",
    "def get_pos_neg_pairs(num_neg=1):\n",
    "    \"\"\"返回一个三元组(u, i, j)构成的list，其中u表示用户，\n",
    "       i表示用户交互过的物体，j表示随机采样的用户没有交互过的物品。\n",
    "    \n",
    "    参数说明\n",
    "    ---\n",
    "    num_neg: 对于每个(u, i)对，采样多少个负样本（没有交互过的物体），默认值为1\n",
    "    \"\"\"\n",
    "    triplets = []\n",
    "    for u, i in train_mat:\n",
    "        for _ in range(num_neg):\n",
    "            j = np.random.randint(num_movies)\n",
    "            while (u, j) in train_mat_set:\n",
    "                j = np.random.randint(num_movies)\n",
    "            triplets.append([u, i, j])\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = get_pos_neg_pairs(num_neg=1)\n",
    "triplets = torch.LongTensor(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 413,   18,  166,  ...,  428,  544,  481],\n",
       "        [2789, 1652, 2556,  ..., 2645, 3265,  898]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives = torch.vstack([triplets[:, 0], triplets[:, 1]]) # 正样本\n",
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 413,   18,  166,  ...,  428,  544,  481],\n",
       "        [3740, 4973, 1550,  ..., 3637, 3552, 7502]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives = torch.vstack([triplets[:, 0], triplets[:, 2]]) # 负样本\n",
    "negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 不依赖PyG直接实现LightGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一节，我们将LightGCN中关于PyG的部分改写为接受稀疏矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGConv(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"前向传播，聚合邻居的信息\"\"\"\n",
    "        return torch.spmm(adj, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ModuleList, Embedding, Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LightGCN(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        alpha = 1. / (num_layers + 1)\n",
    "        self.alpha = torch.tensor([alpha] * (num_layers + 1))\n",
    "\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "        self.convs = ModuleList([LGConv() for _ in range(num_layers)])\n",
    "        self.decoder = torch.nn.Sequential(Linear(2 * embedding_dim, embedding_dim), \n",
    "                                           ReLU(), Linear(embedding_dim, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "\n",
    "    def get_embedding(self, adj):\n",
    "        ###################\n",
    "        #######代码填空#####  \n",
    "        ################## \n",
    "    \n",
    "    def forward(self, adj, edge_label_index):\n",
    "        \"\"\"计算节点对的分数: 对于给定节点对，它们的向量的内积就是其分数\"\"\"\n",
    "        ###################\n",
    "        #######代码填空#####  \n",
    "        ################## \n",
    "\n",
    "    def predict(self, adj, src_index, dst_index):\n",
    "        r\"\"\"预测给定用户对给定物品的结果。\n",
    "        \n",
    "        参数说明\n",
    "        ----\n",
    "        src_index: 用户节点的序号。\n",
    "        dst_index: 待推荐的物品节点的序号。\n",
    "        \"\"\"\n",
    "        ###################\n",
    "        #######代码填空#####  \n",
    "        ################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_nodes = num_users + num_movies\n",
    "model = LightGCN(num_nodes=num_nodes, embedding_dim=32, num_layers=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def average_roc_auc(model, adj):\n",
    "    \"\"\"对每个用户计算AUC并求平均\"\"\"\n",
    "    user_auc_scores = []\n",
    "    for user_id in range(num_users):\n",
    "        pos_item_train_u = pos_item_train[user_id]\n",
    "        pos_item_test_u = pos_item_test[user_id]\n",
    "        \n",
    "        # 去除已经在训练集里的物品\n",
    "        all_item_ids = np.arange(num_users, num_users + num_movies)\n",
    "        items_to_rank = np.setdiff1d(all_item_ids, pos_item_train_u)\n",
    "        \n",
    "        # 对于每个在pos_item_test_u返回1，否则返回0\n",
    "        expected = np.in1d(items_to_rank, pos_item_test_u)\n",
    "        \n",
    "        if np.sum(expected) >= 1:\n",
    "            pred = model.predict(adj, user_id, items_to_rank).detach()\n",
    "            user_auc_scores.append(roc_auc_score(expected, pred))\n",
    "    return sum(user_auc_scores) / len(user_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_pos_item(edge_index):\n",
    "    \"\"\"创建一个字典，字典里存储了每个用户交互过的物品list\"\"\"\n",
    "    pos_item = defaultdict(list)\n",
    "    for x, y in edge_index.numpy().T:\n",
    "        pos_item[x].append(y)\n",
    "    return pos_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_item_train = get_pos_item(train_edge_index)\n",
    "pos_item_test = get_pos_item(test_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机初始化的模型的AUC： 0.5472788783405869\n",
      "Epoch: 020, Loss: 0.00042\n",
      "Epoch: 040, Loss: 0.00039\n",
      "Epoch: 060, Loss: 0.00035\n",
      "Epoch: 080, Loss: 0.00033\n",
      "Epoch: 100, Loss: 0.00030\n",
      "Epoch: 120, Loss: 0.00028\n",
      "Epoch: 140, Loss: 0.00025\n",
      "Epoch: 160, Loss: 0.00023\n",
      "Epoch: 180, Loss: 0.00021\n",
      "Epoch: 200, Loss: 0.00020\n",
      "Epoch: 220, Loss: 0.00018\n",
      "Epoch: 240, Loss: 0.00017\n",
      "Epoch: 260, Loss: 0.00015\n",
      "Epoch: 280, Loss: 0.00014\n",
      "Epoch: 300, Loss: 0.00013\n",
      "训练后的模型的AUC： 0.7466062715954034\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out_pos = model(train_adj_norm, positives)\n",
    "    out_neg = model(train_adj_norm, negatives)\n",
    "    loss = bpr_loss(out_pos, out_neg, model.embedding.weight, lambda_reg=1e-4)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "positives, negatives = positives.to(device), negatives.to(device) \n",
    "\n",
    "\n",
    "print('随机初始化的模型的AUC：', average_roc_auc(model, test_adj_norm))\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    loss = train()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.5f}')\n",
    "        \n",
    "print('训练后的模型的AUC：', average_roc_auc(model, test_adj_norm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
