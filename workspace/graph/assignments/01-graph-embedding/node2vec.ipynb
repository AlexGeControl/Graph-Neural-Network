{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四章作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业我们加强对图嵌入模型的实践，具体地，我们需要利用gensim.Word2Vec模型来实现Node2Vec模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始实践之前，首先请同学回答两个问题：\n",
    "1. Node2Vec模型中的p参数和q参数各代表什么意思？\n",
    "2. 在Node2Vec采样随机游走时，如果我们鼓励随机游走访问之前被采样过的节点，我们应该如何调节p或者q？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码填空 - Node2Vec采样随机游走"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'open' from 'smart_open' (/opt/anaconda/envs/graph/lib/python3.7/site-packages/smart_open/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b5a3cb4c6cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/graph/lib/python3.7/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/graph/lib/python3.7/site-packages/gensim/parsing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from .preprocessing import (remove_stopwords, strip_punctuation, strip_punctuation2,  # noqa:F401\n\u001b[0m\u001b[1;32m      5\u001b[0m                             \u001b[0mstrip_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_numeric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             \u001b[0mstrip_non_alphanum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/graph/lib/python3.7/site-packages/gensim/parsing/preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/graph/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msmart_open\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'open' from 'smart_open' (/opt/anaconda/envs/graph/lib/python3.7/site-packages/smart_open/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec(\n",
    "    adj, \n",
    "    embedding_dim=64, \n",
    "    walk_length=30, \n",
    "    walks_per_node=10,\n",
    "    workers=8, \n",
    "    window_size=10, \n",
    "    num_neg_samples=10, \n",
    "    p=4, \n",
    "    q=1\n",
    "):\n",
    "    \"\"\"\n",
    "    参数说明\n",
    "    -------------\n",
    "    adj : 图的邻接矩阵\n",
    "    embedding_dim : 图嵌入的维度\n",
    "    walk_length : 随机游走的长度\n",
    "    walks_per_node : 每个节点采样多少个随机游走\n",
    "    workers: word2vec模型使用的线程数量\n",
    "    window_size: word2vec模型中使用的窗口大小\n",
    "    num_neg_samples : 负样本的数量\n",
    "    p: node2vec的p参数\n",
    "    q: node2vec的q参数\n",
    "    \"\"\"\n",
    "    # step 1: node2vec second-order random walk\n",
    "    (V, walks) = sample_n2v_random_walks(\n",
    "        adj, \n",
    "        walk_length, \n",
    "        walks_per_node, \n",
    "        p=p, \n",
    "        q=q\n",
    "    )\n",
    "    # step 2: format sampled walks as sentences:\n",
    "    walks = [\n",
    "        list(map(str, walk)) for walk in walks\n",
    "    ]\n",
    "    # step 3: call word2vec:\n",
    "    model = Word2Vec(\n",
    "        sentences=walks, \n",
    "        size=embedding_dim, \n",
    "        negative=num_neg_samples, compute_loss=True\n",
    "    )\n",
    "    # step 4: extract node embeddings:\n",
    "    embedding = np.asarray(\n",
    "        [model.wv.get_vector(str(node_id)) for node_id in range(V)]\n",
    "    )\n",
    "    # done:\n",
    "    return (model, embedding)\n",
    "\n",
    "def sample_n2v_random_walks(adj, walk_length, walks_per_node, p, q):\n",
    "    \"\"\"\n",
    "    返回值的类型\n",
    "    -------\n",
    "    walks : np.ndarray, shape [num_walks * num_nodes, walk_length]\n",
    "        采样后的随机游走\n",
    "    \"\"\"\n",
    "    # format as Compressed Sparse Row matrix, which enables indexing:\n",
    "    adj = sp.csr_matrix(adj)\n",
    "    \n",
    "    # num. of vertices:\n",
    "    V = len(adj.indptr) - 1\n",
    "    \n",
    "    # do node2vec second-order random walk:\n",
    "    random_walks = _n2v_random_walk_numba(\n",
    "        adj.indptr,\n",
    "        adj.indices,\n",
    "        walk_length,\n",
    "        walks_per_node,\n",
    "        p,\n",
    "        q\n",
    "    )\n",
    "    \n",
    "    return (V, random_walks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _n2v_random_walk(\n",
    "    indptr,\n",
    "    indices,\n",
    "    walk_length,\n",
    "    walks_per_node,\n",
    "    p,\n",
    "    q\n",
    "):\n",
    "    # num. of nodes:\n",
    "    V = len(indptr) - 1\n",
    "    \n",
    "    # init output:\n",
    "    final_walks = []\n",
    "    for _ in range(walks_per_node):\n",
    "        for start in range(V):\n",
    "            # init current walk:\n",
    "            (prev, curr) = (start, start)\n",
    "            walk = [curr]\n",
    "            \n",
    "            if walk_length > 1:\n",
    "                # implement node2vector random walk:\n",
    "                for _ in range(walk_length - 1):\n",
    "                    # get previous context:\n",
    "                    prev_context = set(\n",
    "                        indices[indptr[prev]:indptr[prev+1]]\n",
    "                    )\n",
    "                    # get current context:\n",
    "                    curr_context = set(\n",
    "                        indices[indptr[curr]:indptr[curr+1]]\n",
    "                    )\n",
    "                                        \n",
    "                    # set sampling weights:\n",
    "                    context = dict()\n",
    "\n",
    "                    # option 1 -- current neighbor is parent:\n",
    "                    if prev in curr_context:\n",
    "                        context[prev] = (1.0 / p)\n",
    "                        \n",
    "                    # option 2 -- distance to parent is 1:\n",
    "                    for neighbor in curr_context.intersection(prev_context):\n",
    "                        if (neighbor != prev):\n",
    "                            context[neighbor] = 1.0\n",
    "\n",
    "                    # option 3 -- distance to parent is 2:\n",
    "                    for neighbor in curr_context.difference(prev_context):\n",
    "                        if (neighbor != prev):\n",
    "                            context[neighbor] = (1.0 / q)\n",
    "\n",
    "                    #\n",
    "                    # sample by weights:\n",
    "                    #\n",
    "                    # preparation -- get candidate neighbors:\n",
    "                    neighbors = np.asarray(list(context.keys()), dtype=np.int32)\n",
    "                    # preparation -- get sampling probabilities:\n",
    "                    weights = np.asarray(list(context.values()), dtype=np.float32)\n",
    "                    probs = weights / np.sum(weights)\n",
    "                    # sample by probabilities:\n",
    "                    (prev, curr) = (\n",
    "                        curr, np.random.choice(neighbors, size=None, p=probs)\n",
    "                    )\n",
    "                    \n",
    "                    # done:\n",
    "                    walk.append(curr)\n",
    "                    \n",
    "            # done:\n",
    "            final_walks.append(walk)\n",
    "    \n",
    "    # done:\n",
    "    return np.array(final_walks)\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def random_choice(arr, p):\n",
    "    \"\"\" sample node by probability \"\"\"\n",
    "    return arr[np.searchsorted(np.cumsum(p), np.random.random(), side=\"right\")]\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _n2v_random_walk_numba(\n",
    "    indptr,\n",
    "    indices,\n",
    "    walk_length,\n",
    "    walks_per_node,\n",
    "    p,\n",
    "    q\n",
    "):\n",
    "    # num. of nodes:\n",
    "    V = len(indptr) - 1\n",
    "    \n",
    "    for _ in range(walks_per_node):\n",
    "        for start in range(V):\n",
    "            # init current walk:\n",
    "            (prev, curr) = (start, start)\n",
    "            walk = [curr]\n",
    "            \n",
    "            if walk_length > 1:\n",
    "                # implement node2vector random walk:\n",
    "                for _ in range(walk_length - 1):\n",
    "                    # get previous context:\n",
    "                    prev_context = set(\n",
    "                        indices[indptr[prev]:indptr[prev+1]]\n",
    "                    )\n",
    "                    # get current context:\n",
    "                    curr_context = set(\n",
    "                        indices[indptr[curr]:indptr[curr+1]]\n",
    "                    )\n",
    "                                        \n",
    "                    # set sampling weights:\n",
    "                    context = dict()\n",
    "\n",
    "                    # option 1 -- current neighbor is parent:\n",
    "                    if prev in curr_context:\n",
    "                        context[prev] = (1.0 / p)\n",
    "                        \n",
    "                    # option 2 -- distance to parent is 1:\n",
    "                    for neighbor in curr_context.intersection(prev_context):\n",
    "                        if (neighbor != prev):\n",
    "                            context[neighbor] = 1.0\n",
    "\n",
    "                    # option 3 -- distance to parent is 2:\n",
    "                    for neighbor in curr_context.difference(prev_context):\n",
    "                        if (neighbor != prev):\n",
    "                            context[neighbor] = (1.0 / q)\n",
    "\n",
    "                    #\n",
    "                    # sample by weights:\n",
    "                    #\n",
    "                    # preparation -- get candidate neighbors:\n",
    "                    neighbors = np.asarray(list(context.keys()), dtype=np.int32)\n",
    "                    # preparation -- get sampling probabilities:\n",
    "                    weights = np.asarray(list(context.values()), dtype=np.float32)\n",
    "                    probs = weights / np.sum(weights)\n",
    "                    # sample by probabilities:\n",
    "                    (prev, curr) = (\n",
    "                        curr, random_choice(neighbors, p=probs)\n",
    "                    )\n",
    "                    \n",
    "                    # done:\n",
    "                    walk.append(curr)\n",
    "                    \n",
    "            yield walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码填空- 完成测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: here Cora dataset must be manually downloaded and put it here\n",
    "dataset = Planetoid(root='./data', name='Cora')\n",
    "data = dataset[0]\n",
    "adj = to_scipy_sparse_matrix(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fitted model and embeddings:\n",
    "model, embedding = node2vec(adj, embedding_dim=64, p=2, q=0.5)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_embedding(\n",
    "    embeddings, labels, \n",
    "    title\n",
    "):\n",
    "    \"\"\"plot node embedding with tSNE\"\"\"\n",
    "    # fit tSNE:\n",
    "    tsne = TSNE(n_components=2, init='pca', random_state=42)\n",
    "    data = tsne.fit_transform(\n",
    "        normalize(embeddings)\n",
    "    )\n",
    "    \n",
    "    # preprocess for visualization:\n",
    "    x_min, x_max = np.min(data, 0), np.max(data, 0)\n",
    "    data = (data - x_min) / (x_max - x_min)\n",
    "     \n",
    "    # visualize:\n",
    "    (V, _) = data.shape\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 9), dpi=80)\n",
    "    ax = plt.subplot(111)\n",
    "    for node_id in range(V):\n",
    "        plt.text(\n",
    "            data[node_id, 0], data[node_id, 1], str(labels[node_id]),\n",
    "            color=plt.cm.Set1(labels[node_id] / 10.),\n",
    "            fontdict={'weight': 'bold', 'size': 9}\n",
    "        )\n",
    "        \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(title)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plt.show(\n",
    "    plot_embedding(\n",
    "        embedding, np.asarray(data.y), \n",
    "        'Cora Node Embedding Visualization'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_node_classification(\n",
    "    embedding_matrix, labels, \n",
    "    train_mask, test_mask, \n",
    "    normalize_embedding=True, max_iter=1000\n",
    "):      \n",
    "    \"\"\" use single-layer MLP for node label prediction using node2vec embeddings\n",
    "    \"\"\"\n",
    "    # normalize:\n",
    "    X = embedding_matrix\n",
    "    if normalize_embedding:\n",
    "        X = normalize(embedding_matrix)\n",
    "    \n",
    "    # split train-test sets:\n",
    "    X_train, y_train = X[train_mask, :], labels[train_mask]\n",
    "    X_test, y_test = X[test_mask, :], labels[test_mask]\n",
    "    \n",
    "    # build classifier:\n",
    "    clf = MLPClassifier(\n",
    "        random_state=42,\n",
    "        hidden_layer_sizes=[32],\n",
    "        max_iter=max_iter\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction:\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    # get classification report:\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true=y_test, y_pred=preds\n",
    "        )\n",
    "    )\n",
    "    # get accuracy score:\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=preds)\n",
    "    \n",
    "    return preds, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, test_acc = evaluate_node_classification(\n",
    "    embedding, np.asarray(data.y), \n",
    "    np.asarray(data.train_mask), \n",
    "    np.asarray(data.test_mask)\n",
    ")\n",
    "print('Test Acc: %.4f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拓展问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请同学们调节p参数和q参数，对比Node2Vec模型的效果和DeepWalk模型(p=q=1)的效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
