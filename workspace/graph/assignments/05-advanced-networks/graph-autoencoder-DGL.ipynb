{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五课 图上的其他深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的课程中我们介绍了许多图神经网络模型。除了图神经网络，针对于图数据的深度学习模型还有很多，比如图上的自编码器、变分自编码器、循环神经网络和对抗生成网络等。在这一课中，我们对自编码器和变分自编码器进行代码实践。这其中包括了对模型细节和它们的应用的讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 链接预测数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链接预测（link prediction）是常见的与图有关的任务。该任务旨在预测两个节点之间是否存在链接（link），即是否存在边。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于链接预测的数据集，我们可以从节点分类任务的数据集直接构造。比如我们之前常用的Cora数据集，就可以无视掉它的节点标签，把Cora图里面的边当成训练/测试数据。下面我们具体来实践一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "# set device to GPU:\n",
    "device = torch.device('cuda:0')\n",
    "# load dataset:\n",
    "dataset = CoraGraphDataset('./data') # 将数据保存在data文件夹下\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个负采样的函数，获取包含负采样的边的一个新图\n",
    "def construct_negative_graph(graph, k):\n",
    "    \"\"\" construct k negative samples\n",
    "    \"\"\"\n",
    "    src, dst = graph.edges()\n",
    "\n",
    "    neg_src = src.repeat_interleave(k)\n",
    "    neg_dst = torch.randint(0, graph.num_nodes(), (len(src) * k,))\n",
    "    \n",
    "    return dgl.graph((neg_src, neg_dst), num_nodes=graph.num_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2708, num_edges=21112,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_g = construct_negative_graph(g, 2)\n",
    "neg_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个负采样的图的一些内容：\n",
    "* 点的数量和原图相同，点的特征就可以复用原图的点特征。\n",
    "* 边的数量是原图的k倍，上面例子里面k=2。\n",
    "* 边是通过对原图里的源节点随机采样目标节点生成的，所以有很小的概率会出现：和原图相同的边以及重复的边。\n",
    "* 由于是负采样的图，所以所有的边的标签都是0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_edges(graph, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\" train-validaion-test split of graph dataset\n",
    "    \"\"\"\n",
    "    all_edge_idx = np.arange(graph.num_edges())\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    \n",
    "    train_idx_num = int(graph.num_edges() * train_ratio)\n",
    "    val_idx_num = int(graph.num_edges() * val_ratio)\n",
    "    \n",
    "    train_idx = all_edge_idx[: train_idx_num]\n",
    "    val_idx = all_edge_idx[train_idx_num: (train_idx_num + val_idx_num)]\n",
    "    test_idx = all_edge_idx[(train_idx_num + val_idx_num):]\n",
    "    \n",
    "    return train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们按照 85:5:10 的比例把原图和负采样图的边划分成训练/验证/测试的集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_edge_idx, val_pos_edge_idx, test_pos_edge_idx = split_edges(g, train_ratio=0.85, val_ratio=0.05)\n",
    "train_neg_edge_idx, val_neg_edge_idx, test_neg_edge_idx = split_edges(neg_g, train_ratio=0.85, val_ratio=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2505, 9744, 8864, ..., 9848, 5367, 9568])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_edge_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是：\n",
    "* 由于我们的负采样图是可以随时构建的，因此负样本的训练/验证和测试是可以在训练的循环里随时生成。\n",
    "* 通过变量名称，我们就可以设定标签为1还是0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对于图数据的自编码器我们称之为GAE (Graph AutoEncoder)。其包含两个组成部分，编码器（encoder）和解码器（decoder）。图上的编码器常用的就是GCN了；而解码器呢通常用一个内积来表示。具体地，给定两个节点的节点表示，解码器将计算二者的内积，其结果作为两个节点之间存在边的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先构造编码器，由两层GCN组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    \"\"\" deep GCN based encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        \n",
    "        # GCN:\n",
    "        self.conv1 = GraphConv(\n",
    "            in_feats=in_channels, out_feats=2*out_channels, \n",
    "            bias=True, \n",
    "            activation=F.relu, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "        \n",
    "        # GCN:\n",
    "        self.conv2 = GraphConv(\n",
    "            in_feats=2*out_channels, out_feats=out_channels, \n",
    "            bias=True, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.conv1(g, features)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后构建解码器，将给定的节点对映射到[0，1]之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "# 定义内积预测Decoder\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    \"\"\" simple inner product decoder\n",
    "    \"\"\"\n",
    "    def forward(self, graph, h, sigmoid=True):\n",
    "        graph.ndata['h'] = h\n",
    "        graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "        value = graph.edata['score'].sum(dim=1) \n",
    "        return torch.sigmoid(value) if sigmoid else value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAE(torch.nn.Module):\n",
    "    \"\"\" graph autoencoder\n",
    "    \"\"\"\n",
    "    EPSILON = 1e-15 # EPS是一个很小的值，防止取对数的时候出现0值\n",
    "    \n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = InnerProductDecoder()\n",
    "\n",
    "    def encode(self, *args, **kwargs): \n",
    "        \"\"\"编码功能\"\"\"\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        \"\"\"解码功能\"\"\"\n",
    "        return self.decoder(*args, **kwargs)\n",
    "\n",
    "    def recon_loss(self, g, neg_g, h, pos_edge_index, neg_edge_index):\n",
    "        \"\"\"计算正边和负边的二值交叉熵\n",
    "        \n",
    "        参数说明\n",
    "        ----\n",
    "        g: 原始图，即包含正边的图\n",
    "        neg_g：负采样图，即包含负边的图\n",
    "        h: 编码器的输出\n",
    "        pos_edge_index: 正边的边索引\n",
    "        neg_edge_index: 负边的边索引\n",
    "        \"\"\"\n",
    "        # encourage correct prediction of actual edge:\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(g, h)[pos_edge_index] + GAE.EPSILON\n",
    "        ).mean()\n",
    "        \n",
    "        # penalize indication of fabricated edge:\n",
    "        neg_loss = -torch.log(\n",
    "            1 - self.decoder(neg_g, h)[neg_edge_index] + GAE.EPSILON\n",
    "        ).mean()\n",
    "\n",
    "        return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model:\n",
    "in_feats, out_feats = g.ndata['feat'].shape[1], 16\n",
    "model = GAE(GCNEncoder(in_feats, out_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0062,  0.0005,  0.0062,  ...,  0.0014, -0.0021,  0.0010],\n",
       "         [ 0.0046, -0.0008,  0.0048,  ...,  0.0022, -0.0022, -0.0005],\n",
       "         [ 0.0046, -0.0008,  0.0048,  ...,  0.0022, -0.0022, -0.0005],\n",
       "         ...,\n",
       "         [ 0.0009, -0.0005, -0.0013,  ...,  0.0069,  0.0050,  0.0017],\n",
       "         [ 0.0066, -0.0092,  0.0010,  ...,  0.0056,  0.0011, -0.0016],\n",
       "         [ 0.0064, -0.0039,  0.0001,  ...,  0.0070, -0.0030, -0.0099]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([2708, 16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify forward propagation, encoder:\n",
    "latent = model.encode(g, g.ndata['feat'])\n",
    "latent, latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5001, 0.5001,  ..., 0.5000, 0.5000, 0.5000],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify forward propagation, decoder:\n",
    "model.decode(g, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 变分自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变分自编码器和自编码器基本结构相同，都是一个编码器加一个解码器。它们的主要区别是，变分自编码器编码后的隐层表示不再是连续的向量表示，而是通过一个高斯分布来表示。具体地，变分自编码器学习的是这个高斯分布的均值（下面用变量`mu`来表示）和标准差（下面用变量`std`来表示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalGCNEncoder(nn.Module):\n",
    "    MAX_LOGSTD = 10\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GCN:\n",
    "        self.conv1 = GraphConv(\n",
    "            in_feats=in_channels, out_feats=2 * out_channels, \n",
    "            bias=True, \n",
    "            activation=F.relu, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "        \n",
    "        # encoded mu,  \n",
    "        self.conv_mu = GraphConv(\n",
    "            in_feats=2 * out_channels, out_feats=out_channels, \n",
    "            allow_zero_in_degree=True\n",
    "        ) \n",
    "        # encoded log(std):\n",
    "        self.conv_logstd = GraphConv(\n",
    "            in_feats=2 * out_channels, out_feats=out_channels, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.conv1(g, features)\n",
    "        \n",
    "        # get encoded Gaussian distribution:\n",
    "        mu = self.conv_mu(g, h)\n",
    "        logstd = self.conv_logstd(g, h)\n",
    "        \n",
    "        return mu, logstd\n",
    "    \n",
    "class VGAE(GAE): \n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super().__init__(encoder, decoder)\n",
    "\n",
    "    def reparametrize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            # get encoding as [mu - std, mu + std]:\n",
    "            return mu + (2*torch.randn_like(logstd) - 1) * torch.exp(logstd)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        \"\"\" Gaussian distribution encoding\n",
    "        \"\"\"\n",
    "        # get mu and log(std), params of encoded Gaussian:\n",
    "        self.__mu__, self.__logstd__ = self.encoder(*args, **kwargs)\n",
    "        # clamp log(std):\n",
    "        self.__logstd__ = self.__logstd__.clamp(max=VariationalGCNEncoder.MAX_LOGSTD)\n",
    "        # sample from encoded Gaussian:\n",
    "        h = self.reparametrize(self.__mu__, self.__logstd__)\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def kl_loss(self, mu=None, logstd=None):\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "        mu = self.__mu__ if mu is None else mu\n",
    "        logstd = self.__logstd__ if logstd is None else logstd.clamp(max=VariationalGCNEncoder.MAX_LOGSTD)\n",
    "        \n",
    "        # the KL divergence from prior, N(0, I):\n",
    "        return -0.5 * torch.mean(\n",
    "            torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（两个高斯分布的kl loss的公式可以参考该[链接](https://stats.stackexchange.com/questions/234757/how-to-use-kullback-leibler-divergence-if-mean-and-standard-deviation-of-of-two)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGAE(\n",
    "    encoder=VariationalGCNEncoder(in_feats, out_feats),\n",
    "    decoder=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8699,  1.4980, -0.2904,  ..., -1.2899, -4.7583, -3.3002],\n",
       "         [-1.5015,  0.9344, -2.7489,  ..., -3.6698,  1.5656, -0.8233],\n",
       "         [-1.0845, -1.4393, -2.0605,  ..., -2.1850,  0.0485,  1.8520],\n",
       "         ...,\n",
       "         [-2.4298,  0.3927, -0.9932,  ..., -3.0765, -1.0976, -2.3869],\n",
       "         [-3.5497, -0.9164, -0.9434,  ..., -4.3243,  0.3257,  2.5688],\n",
       "         [ 0.6291, -1.2572, -2.2819,  ...,  0.1744, -0.9206, -1.1180]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([2708, 16]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = model.encode(g, g.ndata['feat'])\n",
    "latent, latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+00, 9.6508e-09,  ..., 1.0000e+00, 9.5799e-02,\n",
       "        9.5799e-02], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(g, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练自编码器和变分自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们展示自编码器和变分自编码器的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gae(model, g, neg_g, pos_edge_idx, neg_edge_idx):\n",
    "    \"\"\"训练GAE模型\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    h = model.encode(g, g.ndata['feat'])\n",
    "    loss = model.recon_loss(g, neg_g, h, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train_vgae(model, g, neg_g, pos_edge_idx, neg_edge_idx):\n",
    "    \"\"\"训练VGAE模型，损失函数由重建损失和kl损失组成\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    h = model.encode(g, g.ndata['feat'])\n",
    "    loss = model.recon_loss(g, neg_g, h, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    loss = loss + (1 / g.num_nodes()) * model.kl_loss() # 加上kl loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, g, neg_g, pos_edge_idx, neg_edge_idx):\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    model.eval()\n",
    "    \n",
    "    pos_h = model.encode(g, g.ndata['feat'])\n",
    "    neg_h = model.encode(neg_g, g.ndata['feat'])\n",
    "    pos_y = pos_h.new_ones(pos_edge_idx.size) # 正样本标签\n",
    "    neg_y = neg_h.new_zeros(neg_edge_idx.size) # 负样本标签\n",
    "    y = torch.cat([pos_y, neg_y], dim=0)\n",
    "\n",
    "    pos_pred = model.decoder(g, pos_h)[pos_edge_idx]\n",
    "    neg_pred = model.decoder(neg_g, neg_h)[neg_edge_idx]\n",
    "    pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "\n",
    "    y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
    "\n",
    "    return roc_auc_score(y, pred), average_precision_score(y, pred) # 计算AUC和AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def evaluate_node_classification(\n",
    "    embeddings, labels, \n",
    "    train_mask, test_mask, \n",
    "    normalize_embedding=True, \n",
    "    max_iter=1000\n",
    "):\n",
    "    \"\"\" use single-layer MLP for node label prediction using (variational) graph auto-encoder embeddings\n",
    "    \"\"\"\n",
    "    # normalize:\n",
    "    X = embeddings\n",
    "    if normalize_embedding:\n",
    "        X = normalize(embeddings)\n",
    "    \n",
    "    # split train-test sets:\n",
    "    X_train, y_train = X[train_mask, :], labels[train_mask]\n",
    "    X_test, y_test = X[test_mask, :], labels[test_mask]\n",
    "    \n",
    "    # build classifier:\n",
    "    clf = MLPClassifier(\n",
    "        random_state=42,\n",
    "        hidden_layer_sizes=[32],\n",
    "        max_iter=max_iter\n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    # make prediction:\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    # get classification report:\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true=y_test, y_pred=preds\n",
    "        )\n",
    "    )\n",
    "    # get accuracy score:\n",
    "    test_acc = accuracy_score(y_true=y_test, y_pred=preds)\n",
    "    \n",
    "    return preds, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练GAE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss_train: 0.8407, AUC: 0.9954, AP: 0.9964\n",
      "Epoch: 200, Loss_train: 0.6873, AUC: 0.9981, AP: 0.9977\n",
      "Epoch: 300, Loss_train: 0.5649, AUC: 0.9969, AP: 0.9969\n",
      "Epoch: 400, Loss_train: 0.4703, AUC: 0.9952, AP: 0.9944\n",
      "Epoch: 500, Loss_train: 0.3934, AUC: 0.9917, AP: 0.9897\n",
      "Epoch: 600, Loss_train: 0.3307, AUC: 0.9885, AP: 0.9848\n",
      "Epoch: 700, Loss_train: 0.2777, AUC: 0.9835, AP: 0.9776\n",
      "Epoch: 800, Loss_train: 0.2348, AUC: 0.9779, AP: 0.9692\n",
      "Epoch: 900, Loss_train: 0.1995, AUC: 0.9748, AP: 0.9638\n",
      "Epoch: 1000, Loss_train: 0.1676, AUC: 0.9682, AP: 0.9539\n",
      "Epoch: 1100, Loss_train: 0.1412, AUC: 0.9617, AP: 0.9445\n",
      "Epoch: 1200, Loss_train: 0.1277, AUC: 0.9597, AP: 0.9433\n",
      "Epoch: 1300, Loss_train: 0.1071, AUC: 0.9587, AP: 0.9406\n",
      "Epoch: 1400, Loss_train: 0.0954, AUC: 0.9568, AP: 0.9373\n",
      "Epoch: 1500, Loss_train: 0.0845, AUC: 0.9553, AP: 0.9352\n",
      "Epoch: 1600, Loss_train: 0.0746, AUC: 0.9539, AP: 0.9333\n",
      "Epoch: 1700, Loss_train: 0.0658, AUC: 0.9537, AP: 0.9325\n",
      "Epoch: 1800, Loss_train: 0.0758, AUC: 0.9465, AP: 0.9240\n",
      "Epoch: 1900, Loss_train: 0.0574, AUC: 0.9500, AP: 0.9267\n",
      "Epoch: 2000, Loss_train: 0.0532, AUC: 0.9497, AP: 0.9256\n"
     ]
    }
   ],
   "source": [
    "model = GAE(GCNEncoder(in_feats, out_feats))\n",
    "model = model.to(device)\n",
    "\n",
    "g = g.to(device)\n",
    "neg_g = neg_g.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_gae(model, g, neg_g, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    if epoch % 100 == 0:\n",
    "        auc, ap = test(model, g, neg_g, test_pos_edge_idx, test_neg_edge_idx)\n",
    "        print('Epoch: {:03d}, Loss_train: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.28      0.31       130\n",
      "           1       0.24      0.44      0.31        91\n",
      "           2       0.33      0.34      0.34       144\n",
      "           3       0.52      0.15      0.23       319\n",
      "           4       0.26      0.30      0.28       149\n",
      "           5       0.26      0.43      0.33       103\n",
      "           6       0.14      0.33      0.19        64\n",
      "\n",
      "    accuracy                           0.28      1000\n",
      "   macro avg       0.30      0.32      0.28      1000\n",
      "weighted avg       0.36      0.28      0.28      1000\n",
      "\n",
      "GAE Test Accuracy: 0.2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/graph/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "embedding_gae = model.encode(g, g.ndata['feat']).cpu().detach().numpy()\n",
    "\n",
    "preds, test_acc = evaluate_node_classification(\n",
    "    embedding_gae, g.ndata['label'].cpu().detach().numpy(), \n",
    "    g.ndata['train_mask'].cpu().detach().numpy(), g.ndata['test_mask'].cpu().detach().numpy()\n",
    ")\n",
    "\n",
    "print('GAE Test Accuracy: %.4f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练VGAE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss_train: 1.3937, AUC: 0.6216, AP: 0.5946\n",
      "Epoch: 200, Loss_train: 1.3818, AUC: 0.6626, AP: 0.6815\n",
      "Epoch: 300, Loss_train: 1.2023, AUC: 0.9176, AP: 0.9239\n",
      "Epoch: 400, Loss_train: 1.1174, AUC: 0.9672, AP: 0.9733\n",
      "Epoch: 500, Loss_train: 1.0175, AUC: 0.9838, AP: 0.9878\n",
      "Epoch: 600, Loss_train: 0.9029, AUC: 0.9916, AP: 0.9931\n",
      "Epoch: 700, Loss_train: 0.8507, AUC: 0.9931, AP: 0.9939\n",
      "Epoch: 800, Loss_train: 0.8155, AUC: 0.9930, AP: 0.9931\n",
      "Epoch: 900, Loss_train: 0.7855, AUC: 0.9914, AP: 0.9925\n",
      "Epoch: 1000, Loss_train: 0.7592, AUC: 0.9899, AP: 0.9904\n",
      "Epoch: 1100, Loss_train: 0.7351, AUC: 0.9900, AP: 0.9897\n",
      "Epoch: 1200, Loss_train: 0.7069, AUC: 0.9923, AP: 0.9912\n",
      "Epoch: 1300, Loss_train: 0.6690, AUC: 0.9934, AP: 0.9924\n",
      "Epoch: 1400, Loss_train: 0.6310, AUC: 0.9930, AP: 0.9924\n",
      "Epoch: 1500, Loss_train: 0.5940, AUC: 0.9927, AP: 0.9917\n",
      "Epoch: 1600, Loss_train: 0.5567, AUC: 0.9917, AP: 0.9897\n",
      "Epoch: 1700, Loss_train: 0.5204, AUC: 0.9903, AP: 0.9878\n",
      "Epoch: 1800, Loss_train: 0.4831, AUC: 0.9886, AP: 0.9853\n",
      "Epoch: 1900, Loss_train: 0.4477, AUC: 0.9861, AP: 0.9817\n",
      "Epoch: 2000, Loss_train: 0.4161, AUC: 0.9824, AP: 0.9766\n"
     ]
    }
   ],
   "source": [
    "model = VGAE(VariationalGCNEncoder(in_feats, out_feats))\n",
    "model = model.to(device)\n",
    "\n",
    "g = g.to(device)\n",
    "neg_g = neg_g.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_vgae(model, g, neg_g, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    if epoch % 100 == 0:\n",
    "        auc, ap = test(model, g, neg_g, test_pos_edge_idx, test_neg_edge_idx)\n",
    "        print('Epoch: {:03d}, Loss_train: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.28      0.34       130\n",
      "           1       0.36      0.52      0.42        91\n",
      "           2       0.38      0.44      0.41       144\n",
      "           3       0.52      0.35      0.42       319\n",
      "           4       0.39      0.34      0.36       149\n",
      "           5       0.43      0.58      0.50       103\n",
      "           6       0.14      0.30      0.19        64\n",
      "\n",
      "    accuracy                           0.39      1000\n",
      "   macro avg       0.38      0.40      0.38      1000\n",
      "weighted avg       0.42      0.39      0.39      1000\n",
      "\n",
      "GAE Test Accuracy: 0.3870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/graph/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "embedding_gae = model.encode(g, g.ndata['feat']).cpu().detach().numpy()\n",
    "\n",
    "preds, test_acc = evaluate_node_classification(\n",
    "    embedding_gae, g.ndata['label'].cpu().detach().numpy(), \n",
    "    g.ndata['train_mask'].cpu().detach().numpy(), g.ndata['test_mask'].cpu().detach().numpy()\n",
    ")\n",
    "\n",
    "print('GAE Test Accuracy: %.4f' % test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
