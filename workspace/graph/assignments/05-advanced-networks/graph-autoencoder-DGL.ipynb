{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五课 图上的其他深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的课程中我们介绍了许多图神经网络模型。除了图神经网络，针对于图数据的深度学习模型还有很多，比如图上的自编码器、变分自编码器、循环神经网络和对抗生成网络等。在这一课中，我们对自编码器和变分自编码器进行代码实践。这其中包括了对模型细节和它们的应用的讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 链接预测数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链接预测（link prediction）是常见的与图有关的任务。该任务旨在预测两个节点之间是否存在链接（link），即是否存在边。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于链接预测的数据集，我们可以从节点分类任务的数据集直接构造。比如我们之前常用的Cora数据集，就可以无视掉它的节点标签，把Cora图里面的边当成训练/测试数据。下面我们具体来实践一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "\n",
    "# set device to GPU:\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# load dataset:\n",
    "dataset = CoraGraphDataset('./data') # 将数据保存在data文件夹下\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个负采样的函数，获取包含负采样的边的一个新图\n",
    "def construct_negative_graph(graph, k):\n",
    "    \"\"\" construct k negative samples\n",
    "    \"\"\"\n",
    "    src, dst = graph.edges()\n",
    "\n",
    "    neg_src = src.repeat_interleave(k)\n",
    "    neg_dst = torch.randint(0, graph.num_nodes(), (len(src) * k,))\n",
    "    \n",
    "    return dgl.graph((neg_src, neg_dst), num_nodes=graph.num_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2708, num_edges=21112,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_g = construct_negative_graph(g, 2)\n",
    "neg_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个负采样的图的一些内容：\n",
    "* 点的数量和原图相同，点的特征就可以复用原图的点特征。\n",
    "* 边的数量是原图的k倍，上面例子里面k=2。\n",
    "* 边是通过对原图里的源节点随机采样目标节点生成的，所以有很小的概率会出现：和原图相同的边以及重复的边。\n",
    "* 由于是负采样的图，所以所有的边的标签都是0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_edges(graph, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\" train-validaion-test split of graph dataset\n",
    "    \"\"\"\n",
    "    all_edge_idx = np.arange(graph.num_edges())\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    \n",
    "    train_idx_num = int(graph.num_edges() * train_ratio)\n",
    "    val_idx_num = int(graph.num_edges() * val_ratio)\n",
    "    \n",
    "    train_idx = all_edge_idx[: train_idx_num]\n",
    "    val_idx = all_edge_idx[train_idx_num: (train_idx_num + val_idx_num)]\n",
    "    test_idx = all_edge_idx[(train_idx_num + val_idx_num):]\n",
    "    \n",
    "    return train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们按照 85:5:10 的比例把原图和负采样图的边划分成训练/验证/测试的集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_edge_idx, val_pos_edge_idx, test_pos_edge_idx = split_edges(g, train_ratio=0.85, val_ratio=0.05)\n",
    "train_neg_edge_idx, val_neg_edge_idx, test_neg_edge_idx = split_edges(neg_g, train_ratio=0.85, val_ratio=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9233, 7324,  228, ...,  926, 1506, 8939])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_edge_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是：\n",
    "* 由于我们的负采样图是可以随时构建的，因此负样本的训练/验证和测试是可以在训练的循环里随时生成。\n",
    "* 通过变量名称，我们就可以设定标签为1还是0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对于图数据的自编码器我们称之为GAE (Graph AutoEncoder)。其包含两个组成部分，编码器（encoder）和解码器（decoder）。图上的编码器常用的就是GCN了；而解码器呢通常用一个内积来表示。具体地，给定两个节点的节点表示，解码器将计算二者的内积，其结果作为两个节点之间存在边的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先构造编码器，由两层GCN组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    \"\"\" deep GCN based encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        \n",
    "        # GCN:\n",
    "        self.conv1 = GraphConv(\n",
    "            in_feats=in_channels, out_feats=2*out_channels, \n",
    "            bias=True, \n",
    "            activation=F.relu, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "        \n",
    "        # GCN:\n",
    "        self.conv2 = GraphConv(\n",
    "            in_feats=2*out_channels, out_feats=out_channels, \n",
    "            bias=True, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.conv1(g, features)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后构建解码器，将给定的节点对映射到[0，1]之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "# 定义内积预测Decoder\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    \"\"\" simple inner product decoder\n",
    "    \"\"\"\n",
    "    def forward(self, graph, h, sigmoid=True):\n",
    "        graph.ndata['h'] = h\n",
    "        graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "        value = graph.edata['score'].sum(dim=1) \n",
    "        return torch.sigmoid(value) if sigmoid else value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAE(torch.nn.Module):\n",
    "    \"\"\" graph autoencoder\n",
    "    \"\"\"\n",
    "    EPSILON = 1e-15 # EPS是一个很小的值，防止取对数的时候出现0值\n",
    "    \n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = InnerProductDecoder()\n",
    "\n",
    "    def encode(self, *args, **kwargs): \n",
    "        \"\"\"编码功能\"\"\"\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        \"\"\"解码功能\"\"\"\n",
    "        return self.decoder(*args, **kwargs)\n",
    "\n",
    "    def recon_loss(self, g, neg_g, h, pos_edge_index, neg_edge_index):\n",
    "        \"\"\"计算正边和负边的二值交叉熵\n",
    "        \n",
    "        参数说明\n",
    "        ----\n",
    "        g: 原始图，即包含正边的图\n",
    "        neg_g：负采样图，即包含负边的图\n",
    "        h: 编码器的输出\n",
    "        pos_edge_index: 正边的边索引\n",
    "        neg_edge_index: 负边的边索引\n",
    "        \"\"\"\n",
    "        # encourage correct prediction of actual edge:\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(g, h)[pos_edge_index] + GAE.EPSILON\n",
    "        ).mean()\n",
    "        \n",
    "        # penalize indication of fabricated edge:\n",
    "        neg_loss = -torch.log(\n",
    "            1 - self.decoder(neg_g, h)[neg_edge_index] + GAE.EPSILON\n",
    "        ).mean()\n",
    "\n",
    "        return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model:\n",
    "in_feats, out_feats = g.ndata['feat'].shape[1], 16\n",
    "model = GAE(GCNEncoder(in_feats, out_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0004, -0.0065, -0.0013,  ..., -0.0011, -0.0008, -0.0009],\n",
       "         [ 0.0013, -0.0061, -0.0015,  ..., -0.0011,  0.0014,  0.0005],\n",
       "         [ 0.0013, -0.0061, -0.0015,  ..., -0.0011,  0.0014,  0.0005],\n",
       "         ...,\n",
       "         [-0.0037, -0.0017, -0.0040,  ..., -0.0014, -0.0021, -0.0034],\n",
       "         [-0.0154, -0.0038, -0.0116,  ..., -0.0047, -0.0047, -0.0010],\n",
       "         [ 0.0012, -0.0008, -0.0014,  ...,  0.0028,  0.0041,  0.0047]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([2708, 16]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify forward propagation, encoder:\n",
    "latent = model.encode(g, g.ndata['feat'])\n",
    "latent, latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.5000,  ..., 0.5001, 0.5000, 0.5000],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify forward propagation, decoder:\n",
    "model.decode(g, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 变分自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变分自编码器和自编码器基本结构相同，都是一个编码器加一个解码器。它们的主要区别是，变分自编码器编码后的隐层表示不再是连续的向量表示，而是通过一个高斯分布来表示。具体地，变分自编码器学习的是这个高斯分布的均值（下面用变量`mu`来表示）和标准差（下面用变量`std`来表示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalGCNEncoder(nn.Module):\n",
    "    MAX_LOGSTD = 10\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GCN:\n",
    "        self.conv1 = GraphConv(\n",
    "            in_feats=in_channels, out_feats=2 * out_channels, \n",
    "            bias=True, \n",
    "            activation=F.relu, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "        \n",
    "        # encoded mu,  \n",
    "        self.conv_mu = GraphConv(\n",
    "            in_feats=2 * out_channels, out_feats=out_channels, \n",
    "            allow_zero_in_degree=True\n",
    "        ) \n",
    "        # encoded log(std):\n",
    "        self.conv_logstd = GraphConv(\n",
    "            in_feats=2 * out_channels, out_feats=out_channels, \n",
    "            allow_zero_in_degree=True\n",
    "        )\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.conv1(g, features)\n",
    "        \n",
    "        # get encoded Gaussian distribution:\n",
    "        mu = self.conv_mu(g, h)\n",
    "        logstd = self.conv_logstd(g, h)\n",
    "        \n",
    "        return mu, logstd\n",
    "    \n",
    "class VGAE(GAE): \n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super().__init__(encoder, decoder)\n",
    "\n",
    "    def reparametrize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            # get encoding as [mu - std, mu + std]:\n",
    "            return mu + (2*torch.randn_like(logstd) - 1) * torch.exp(logstd)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        \"\"\" Gaussian distribution encoding\n",
    "        \"\"\"\n",
    "        # get mu and log(std), params of encoded Gaussian:\n",
    "        self.__mu__, self.__logstd__ = self.encoder(*args, **kwargs)\n",
    "        # clamp log(std):\n",
    "        self.__logstd__ = self.__logstd__.clamp(max=VariationalGCNEncoder.MAX_LOGSTD)\n",
    "        # sample from encoded Gaussian:\n",
    "        h = self.reparametrize(self.__mu__, self.__logstd__)\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def kl_loss(self, mu=None, logstd=None):\n",
    "        \"\"\"\n",
    "        \"\"\" \n",
    "        mu = self.__mu__ if mu is None else mu\n",
    "        logstd = self.__logstd__ if logstd is None else logstd.clamp(max=VariationalGCNEncoder.MAX_LOGSTD)\n",
    "        \n",
    "        # the KL divergence from prior, N(0, I):\n",
    "        return -0.5 * torch.mean(\n",
    "            torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（两个高斯分布的kl loss的公式可以参考该[链接](https://stats.stackexchange.com/questions/234757/how-to-use-kullback-leibler-divergence-if-mean-and-standard-deviation-of-of-two)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGAE(\n",
    "    encoder=VariationalGCNEncoder(in_channels, out_channels),\n",
    "    decoder=None\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-06116cb3f7d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-eceea8709206>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# get mu and log(std), params of encoded Gaussian:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mu__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__logstd__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m# clamp log(std):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__logstd__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__logstd__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVariationalGCNEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_LOGSTD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/graph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-eceea8709206>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, features)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# get encoded Gaussian distribution:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/graph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/graph/lib/python3.7/site-packages/dgl/nn/pytorch/conv/graphconv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;31m# mult W first to reduce the feature size for aggregation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mfeat_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrcdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_src\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "latent = model.encode(g, g.ndata['feat'])\n",
    "latent, latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decode(g, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练自编码器和变分自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们展示自编码器和变分自编码器的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gae(model, g, neg_g, pos_edge_idx, neg_edge_idx):\n",
    "    \"\"\"训练GAE模型\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    h = model.encode(g, g.ndata['feat'])\n",
    "    loss = model.recon_loss(g, neg_g, h, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train_vgae(model, g, neg_g, pos_edge_idx, neg_edge_idx):\n",
    "    \"\"\"训练VGAE模型，损失函数由重建损失和kl损失组成\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    h = model.encode(g, g.ndata['feat'])\n",
    "    loss = model.recon_loss(g, neg_g, h, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    loss = loss + (1 / g.num_nodes()) * model.kl_loss() # 加上kl loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, g, neg_g, pos_edge_idx, neg_edge_idx):\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    model.eval()\n",
    "    \n",
    "    pos_h = model.encode(g, g.ndata['feat'])\n",
    "    neg_h = model.encode(neg_g, g.ndata['feat'])\n",
    "    pos_y = pos_h.new_ones(pos_edge_idx.size) # 正样本标签\n",
    "    neg_y = neg_h.new_zeros(neg_edge_idx.size) # 负样本标签\n",
    "    y = torch.cat([pos_y, neg_y], dim=0)\n",
    "\n",
    "    pos_pred = model.decoder(g, pos_h)[pos_edge_idx]\n",
    "    neg_pred = model.decoder(neg_g, neg_h)[neg_edge_idx]\n",
    "    pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "\n",
    "    y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
    "\n",
    "    return roc_auc_score(y, pred), average_precision_score(y, pred) # 计算AUC和AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练GAE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss_train: 0.8731, AUC: 0.9938, AP: 0.9953\n",
      "Epoch: 200, Loss_train: 0.7226, AUC: 0.9960, AP: 0.9964\n",
      "Epoch: 300, Loss_train: 0.6000, AUC: 0.9938, AP: 0.9947\n",
      "Epoch: 400, Loss_train: 0.4872, AUC: 0.9905, AP: 0.9901\n",
      "Epoch: 500, Loss_train: 0.3914, AUC: 0.9863, AP: 0.9851\n",
      "Epoch: 600, Loss_train: 0.3177, AUC: 0.9791, AP: 0.9757\n",
      "Epoch: 700, Loss_train: 0.2567, AUC: 0.9722, AP: 0.9660\n",
      "Epoch: 800, Loss_train: 0.2110, AUC: 0.9659, AP: 0.9574\n",
      "Epoch: 900, Loss_train: 0.1707, AUC: 0.9608, AP: 0.9492\n",
      "Epoch: 1000, Loss_train: 0.1385, AUC: 0.9534, AP: 0.9388\n",
      "Epoch: 1100, Loss_train: 0.1276, AUC: 0.9499, AP: 0.9353\n",
      "Epoch: 1200, Loss_train: 0.1160, AUC: 0.9499, AP: 0.9343\n",
      "Epoch: 1300, Loss_train: 0.1068, AUC: 0.9483, AP: 0.9312\n",
      "Epoch: 1400, Loss_train: 0.0981, AUC: 0.9468, AP: 0.9290\n",
      "Epoch: 1500, Loss_train: 0.0897, AUC: 0.9450, AP: 0.9259\n",
      "Epoch: 1600, Loss_train: 0.0815, AUC: 0.9432, AP: 0.9230\n",
      "Epoch: 1700, Loss_train: 0.0738, AUC: 0.9413, AP: 0.9200\n",
      "Epoch: 1800, Loss_train: 0.0666, AUC: 0.9400, AP: 0.9183\n",
      "Epoch: 1900, Loss_train: 0.0600, AUC: 0.9395, AP: 0.9175\n",
      "Epoch: 2000, Loss_train: 0.0546, AUC: 0.9388, AP: 0.9160\n"
     ]
    }
   ],
   "source": [
    "model = GAE(GCNEncoder(in_channels, out_channels))\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    " \n",
    "\n",
    "epochs = 2000\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_gae(model, g, neg_g, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    if epoch % 100 == 0:\n",
    "        auc, ap = test(model, g, neg_g, test_pos_edge_idx, test_neg_edge_idx)\n",
    "        print('Epoch: {:03d}, Loss_train: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练VGAE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss_train: 1.1905, AUC: 0.9409, AP: 0.9442\n",
      "Epoch: 200, Loss_train: 0.9519, AUC: 0.9952, AP: 0.9964\n",
      "Epoch: 300, Loss_train: 0.8732, AUC: 0.9965, AP: 0.9970\n",
      "Epoch: 400, Loss_train: 0.7789, AUC: 0.9956, AP: 0.9959\n",
      "Epoch: 500, Loss_train: 0.7062, AUC: 0.9949, AP: 0.9956\n",
      "Epoch: 600, Loss_train: 0.6481, AUC: 0.9918, AP: 0.9934\n",
      "Epoch: 700, Loss_train: 0.5987, AUC: 0.9927, AP: 0.9933\n",
      "Epoch: 800, Loss_train: 0.5571, AUC: 0.9927, AP: 0.9922\n",
      "Epoch: 900, Loss_train: 0.5202, AUC: 0.9936, AP: 0.9919\n",
      "Epoch: 1000, Loss_train: 0.4823, AUC: 0.9923, AP: 0.9900\n",
      "Epoch: 1100, Loss_train: 0.4491, AUC: 0.9903, AP: 0.9877\n",
      "Epoch: 1200, Loss_train: 0.4156, AUC: 0.9894, AP: 0.9863\n",
      "Epoch: 1300, Loss_train: 0.3848, AUC: 0.9884, AP: 0.9841\n",
      "Epoch: 1400, Loss_train: 0.3548, AUC: 0.9877, AP: 0.9826\n",
      "Epoch: 1500, Loss_train: 0.3292, AUC: 0.9854, AP: 0.9797\n",
      "Epoch: 1600, Loss_train: 0.3029, AUC: 0.9836, AP: 0.9765\n",
      "Epoch: 1700, Loss_train: 0.2785, AUC: 0.9831, AP: 0.9752\n",
      "Epoch: 1800, Loss_train: 0.2566, AUC: 0.9816, AP: 0.9723\n",
      "Epoch: 1900, Loss_train: 0.2378, AUC: 0.9793, AP: 0.9687\n",
      "Epoch: 2000, Loss_train: 0.2201, AUC: 0.9774, AP: 0.9653\n"
     ]
    }
   ],
   "source": [
    "model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    " \n",
    "epochs = 2000\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_vgae(model, g, neg_g, train_pos_edge_idx, train_neg_edge_idx)\n",
    "    if epoch % 100 == 0:\n",
    "        auc, ap = test(model, g, neg_g, test_pos_edge_idx, test_neg_edge_idx)\n",
    "        print('Epoch: {:03d}, Loss_train: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
